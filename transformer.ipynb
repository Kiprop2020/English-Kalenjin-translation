{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"vpASms6W4YU_","executionInfo":{"status":"ok","timestamp":1686230364687,"user_tz":-180,"elapsed":5128,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"}}},"outputs":[],"source":["import numpy\n","import random\n","import json\n","\n","import re\n","import string\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import load_model"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rwzpmzDb4n99","executionInfo":{"status":"ok","timestamp":1686230383043,"user_tz":-180,"elapsed":18368,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"}},"outputId":"d531faac-6970-4040-8ae6-229e59a5c250"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1686230383046,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"Ux7njpN9hGU5"},"outputs":[],"source":["title = 'eng_niv_kal'\n","\n","project_folder = '/content/drive/MyDrive/ENG-KAL_translation'\n","# version = 'niv'\n","# eng_folder = 'English texts/' + version\n","# kal_folder = 'Kalenjin texts'\n","eng_kal_txt = project_folder + '/engnivkal_202306081459.txt'"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":717,"status":"ok","timestamp":1686230388472,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"c_iPHZpZilXF"},"outputs":[],"source":["with open(eng_kal_txt, 'r') as fjson:\n","    data = json.load(fjson)\n","len_data = len(data)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"md39lRF54YVE","executionInfo":{"status":"ok","timestamp":1686230388473,"user_tz":-180,"elapsed":7,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"}},"outputId":"07fefd7b-fd5e-4346-f290-c7ba65c20b1e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['in the beginning god created the heavens and the earth',\n"," '[start] eng  taunet  ko ki toi kamuktaindet koyai kipsengwet ak ng wony [end]']"]},"metadata":{},"execution_count":5}],"source":["data[0]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":347,"status":"ok","timestamp":1686230396844,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"2GAZ6kMok7ei","outputId":"e194ee69-5ef0-40a6-c94e-2c6a2ec4d4b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["21275 6079 3039\n","['let me tell you a riddle   samson said to them   if you can give me the answer within the seven days of the feast  i will give you thirty linen garments and thirty sets of clothes', '[start] ki mwachi  samson icheek koleenji   ogany amwaiwok tangochet  ngot omuuchi omwaiwo eng  peetuusyek tisap che po igoorto  ak onai kiit ne ibooru  agoonok anyun ngoroik sosom che po katanit ak ngoroik sosom che kiwalawali [end]']\n","['is it not to share your food with the hungryand to provide the poor wanderer with shelterâ€”when you see the naked  to clothe them and not to turn away from your own flesh and blood', '[start] tos ma kebae amitwogiguk che amei rubeet  ak imutu kibananook che kigetimda  kobwa koong ung   ye igeer ne mi aach ngor  ituch  ameung egei piik che po oreng ung [end]']\n","['whoever robs their father and drives out their motheris a child who brings shame and disgrace', '[start] ibu kalilanet ak teweernateet weeriit ne iseeri tuguugap kwandanyi ak koon kamennyi [end]']\n"]}],"source":["train_len = round(.7 * len(data))\n","val_len = round(.2 * len(data))\n","\n","random.shuffle(data)\n","train_pairs = data[:train_len]\n","val_pairs = data[train_len:train_len + val_len]\n","test_pairs = data[train_len + val_len:]\n","print(len(train_pairs), len(val_pairs), len(test_pairs))\n","print(train_pairs[15])\n","print(val_pairs[10])\n","print(test_pairs[14])"]},{"cell_type":"markdown","metadata":{"id":"flhTStO3lSai"},"source":["Define the train pairs"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":6391,"status":"ok","timestamp":1686230407816,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"9Rm1uvKiQsvk"},"outputs":[],"source":["strip_chars = string.punctuation\n","strip_chars = strip_chars.replace('(', '').replace(')', '')\n","\n","# def custom_standardization(input_string):\n","#     lowercase = tf.strings.lower(input_string)\n","#     return tf.strings.regex_replace(\n","#         lowercase, f'[{re.escape(strip_chars)}]', ''\n","#     )\n","\n","vocab_size = 15000\n","sequence_length = 20\n","\n","source_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode='int',\n","    output_sequence_length=sequence_length,\n",")\n","\n","target_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode='int',\n","    output_sequence_length=sequence_length + 1,\n","    standardize='lower_and_strip_punctuation'\n",")\n","\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_kal_texts = [pair[1] for pair in train_pairs]\n","source_vectorization.adapt(train_eng_texts)\n","target_vectorization.adapt(train_kal_texts)"]},{"cell_type":"markdown","metadata":{"id":"o2GqN5wxlGm2"},"source":["Prepare the datasets for the translation task"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":592,"status":"ok","timestamp":1686230408403,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"5DSp5sB-lFfx"},"outputs":[],"source":["batch_size = 32\n","\n","def format_dataset(eng, kal):\n","    eng = source_vectorization(eng)\n","    kal = target_vectorization(kal)\n","    return(\n","        {\n","            'english': eng,\n","            'kalenjin': kal[:, :-1]\n","        }, kal[:, 1:]\n","    )\n","\n","def make_dataset(pairs):\n","    eng_texts, kal_texts = zip(*pairs)\n","    eng_texts, kal_texts = list(eng_texts), list(kal_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, kal_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n","    return dataset.shuffle(3).prefetch(1).cache()\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1686230408404,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"o5TQSxCks5j7","outputId":"ce8e3d59-edcc-490b-c4f8-fad8e20af6d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["inputs['english'].shape: (32, 20)\n","inputs['kalenjin'].shape: (32, 20)\n","targets.shape: (32, 20)\n"]}],"source":["for inputs, targets, in train_ds.take(1):\n","    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n","    print(f\"inputs['kalenjin'].shape: {inputs['kalenjin'].shape}\")\n","    print(f\"targets.shape: {targets.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"Wz0EE_3EtxUH"},"source":["TRANSFORMER NETWORK"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1686230408405,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"1de3zCjjtv5Q"},"outputs":[],"source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim                          # Size of the input token vectors\n","        self.dense_dim = dense_dim                          # Size of the inner dense layer\n","        self.num_heads = num_heads                          # Number of attention heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","             layers.Dense(dense_dim, activation='relu'),\n","             layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","\n","    def call(self, inputs, mask=None):                      # Computation comes here\n","        if mask is not None:\n","            mask = mask[:, tf.newaxis, :]                   # Expand the mask to 3D\n","        attention_output = self.attention(\n","            inputs, inputs, attention_mask=mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):                                   # Implement serialization so the model can be saved\n","        config = super().get_config()\n","        config.update(\n","            {\n","                'embed_dim': self.embed_dim,\n","                'num_heads': self.num_heads,\n","                'dense_dim': self.dense_dim             \n","            }\n","        )\n","        return config"]},{"cell_type":"markdown","metadata":{"id":"BBGN3cxNt3cJ"},"source":["POSITIONAL EMBEDDING"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686230408741,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"J_8opA5_t6Aw"},"outputs":[],"source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(                   # Prepare and Embedding layer for the token indices\n","            input_dim=input_dim, output_dim=output_dim\n","        )\n","        self.position_embeddings = layers.Embedding(                # Prepare and Embedding layer for the token positions\n","            input_dim=sequence_length, output_dim=output_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions                 # Add both embedding vectors together\n","\n","    def compute_mask(self, inputs, mask=None):                      # Generate a mask so we can ignore padding 0s in the inputs\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):                                           # Implement serialization so we can save the model\n","        config = super().get_config()\n","        config.update(\n","            {\n","                'output_dim': self.output_dim,\n","                'sequence_length': self.sequence_length,\n","                'input_dim': self.input_dim\n","            }\n","        )\n","        return config"]},{"cell_type":"markdown","metadata":{"id":"7Kx63POkt9Xi"},"source":["DECODER NETWORK"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1686230410722,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"DQ1qPHonuADR"},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","             layers.Dense(dense_dim, activation='relu'),\n","             layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.support_masking = True\n","    \n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype='int32')\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1),\n","             tf.constant([1, 1], dtype=tf.int32)], axis=0\n","        )\n","        return tf.tile(mask, mult)\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(\n","                mask[:, tf.newaxis, :], dtype='int32'\n","            )\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=causal_mask\n","        )\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        attention_output_2 = self.attention_2(\n","            query=attention_output_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        attention_output_2 = self.layernorm_2(\n","            attention_output_1 + attention_output_2\n","        )\n","        proj_output = self.dense_proj(attention_output_2)\n","        return self.layernorm_3(attention_output_2 + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                'embed_dim': self.embed_dim,\n","                'num_heads': self.num_heads,\n","                'dense_dim': self.dense_dim\n","            }\n","        )\n","        return config"]},{"cell_type":"markdown","metadata":{"id":"Ns44ckJcuB0K"},"source":["PUTTING ALL TOGETHER"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":978,"status":"ok","timestamp":1686233275053,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"y946nx-9t8jG"},"outputs":[],"source":["embed_dim = 256\n","dense_dim = 2048\n","num_heads = 18\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype='int64', name='english')\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype='int64', name='kalenjin')\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation='softmax')(x)\n","\n","transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"markdown","metadata":{"id":"VpIApHeqwFmb"},"source":["Compile the Transformer Network"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":338,"status":"ok","timestamp":1686233280540,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"q_HKkt6A1cHY"},"outputs":[],"source":["import datetime"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686233282521,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"_AjKk7bV1hwI"},"outputs":[],"source":["now = datetime.datetime.now()\n","running_time = now.strftime(\"%Y%m%d%H%M\")"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":489,"status":"ok","timestamp":1686233284632,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"_5BxJXrSv635"},"outputs":[],"source":["transformer.compile(\n","    optimizer='rmsprop',\n","    loss='sparse_categorical_crossentropy',\n","    metrics='accuracy'\n",")\n","\n","callbacks = keras.callbacks.ModelCheckpoint(f'{project_folder}/models/eng_to_kal_transformer_{running_time}.keras', save_best_only=True)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227150,"status":"ok","timestamp":1686233513692,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"xgV553aLwq-S","outputId":"82c63258-9e18-4243-c695-65e5c63b0ce0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","665/665 [==============================] - 63s 83ms/step - loss: 4.9675 - accuracy: 0.2911 - val_loss: 4.3618 - val_accuracy: 0.3358\n","Epoch 2/5\n","665/665 [==============================] - 41s 61ms/step - loss: 4.4168 - accuracy: 0.3390 - val_loss: 4.1545 - val_accuracy: 0.3546\n","Epoch 3/5\n","665/665 [==============================] - 42s 63ms/step - loss: 4.1898 - accuracy: 0.3615 - val_loss: 4.0049 - val_accuracy: 0.3708\n","Epoch 4/5\n","665/665 [==============================] - 41s 62ms/step - loss: 4.0213 - accuracy: 0.3797 - val_loss: 3.9504 - val_accuracy: 0.3774\n","Epoch 5/5\n","665/665 [==============================] - 40s 61ms/step - loss: 3.8872 - accuracy: 0.3956 - val_loss: 3.9024 - val_accuracy: 0.3852\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fde583cc790>"]},"metadata":{},"execution_count":42}],"source":["epochs = 5\n","transformer.fit(train_ds,\n","                epochs=epochs,\n","                validation_data=val_ds,\n","                callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"vhsAHKavw805"},"source":["Test translation accuracy"]},{"cell_type":"code","source":["len(kal_vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UER79ooeMiHE","executionInfo":{"status":"ok","timestamp":1686230947171,"user_tz":-180,"elapsed":11,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"}},"outputId":"85facebb-4b7a-4880-a0fb-787b8d6a5a33"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15000"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6613,"status":"ok","timestamp":1686233575217,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"KstEwTuWw7dK","outputId":"a244fc2b-28f4-4fc2-e4a1-563f5c1c9f80"},"outputs":[{"output_type":"stream","name":"stdout","text":["*******************************************************************\n","each of us should please our neighbors for their good  to build them up\n","[start] ak ki [UNK] chii age tugul eng icheek kole [UNK] end         \n","*******************************************************************\n","you are my war club my weapon for battleâ€”with you i shatter nations with you i destroy kingdoms\n","[start] ki [UNK] anyun ak [UNK] ak [UNK] amu ki [UNK] ak [UNK] end       \n","*******************************************************************\n","â€˜so do not be afraid  jacob my servant do not be dismayed  israel â€™declares the lord â€˜i will surely save you out of a distant place your descendants from the land of their exile jacob will again have peace and security and no one will make him afraid\n","[start] amu ma uu nooto kamuktaindet ne toroor jakobo ak jakobo ak ki le israel amu ma [UNK] israel end \n","*******************************************************************\n","remove your scourge from me i am overcome by the blow of your hand\n","[start] [UNK] eng taing ung ak [UNK] eng taing ung ne po aeng end       \n","*******************************************************************\n","after that  they presented the regular burnt offerings  the new moon sacrifices and the sacrifices for all the appointed sacred festivals of the lord  as well as those brought as freewill offerings to the lord\n","[start] ye kingo [UNK] icheek kamuktaindet ne toroor kagoochinet ne po kagoochinet ne po kagoochinet ne kibeelei ak kogoochi kamuktaindet ne\n","*******************************************************************\n","he followed the advice of the young men and said   my father made your yoke heavy  i will make it even heavier  my father scourged you with whips  i will scourge you with scorpions\n","[start] ki leenji elisha ineendet [UNK] ak [UNK] ak icheek ak koleenji [UNK] kamuktaindet ne toroor end    \n"]}],"source":["kal_vocab = target_vectorization.get_vocabulary()\n","kal_index_lookup = dict(zip(range(len(kal_vocab)), kal_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = '[start]'\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization(\n","            [decoded_sentence]\n","        )[:, :-1]\n","        predictions = transformer(\n","            [tokenized_input_sentence, tokenized_target_sentence]\n","        )\n","        sampled_token_index = numpy.argmax(predictions[0, i, :])\n","        sampled_token = kal_index_lookup[sampled_token_index]\n","        decoded_sentence += ' ' + sampled_token\n","        if sampled_token == '[end]':\n","            break\n","    return decoded_sentence\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(6):\n","    input_sentence = random.choice(test_eng_texts)\n","    print('*******************************************************************')\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"]},{"cell_type":"markdown","metadata":{"id":"-xdHAYlRX7vm"},"source":["Save the training data and metadata for later use"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1686234160458,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"X1_YyiF2P-OD"},"outputs":[],"source":["test_sentences = [\n","'where are you going',\n","'come home today',\n","'what is your name',\n","'how much love',\n","'God is calling you to himself'\n"," ]\n","\n","test_sentences = ['[start] ' + item + ' [end]' for item in test_sentences]"]},{"cell_type":"code","source":[],"metadata":{"id":"ryGEBZvIMIEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5768,"status":"ok","timestamp":1686234169192,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"},"user_tz":-180},"id":"gza9ZTGOSb2A","outputId":"74c3c6cb-1281-47dd-d99c-34e610c53359"},"outputs":[{"output_type":"stream","name":"stdout","text":["*******************************************************************\n","[start] where are you going [end]\n","[start] [UNK] end                  \n","*******************************************************************\n","[start] come home today [end]\n","[start] [UNK] anyun daudi ak [UNK] end              \n","*******************************************************************\n","[start] what is your name [end]\n","[start] [UNK] end                  \n","*******************************************************************\n","[start] how much love [end]\n","[start] [UNK] [UNK] [UNK] end                \n","*******************************************************************\n","[start] God is calling you to himself [end]\n","[start] [UNK] kamuktaindet end                 \n"]}],"source":["for i in range(len(test_sentences)):\n","    input_sentence = test_sentences[i]\n","    print('*******************************************************************')\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0aelqM_Y41k3"},"outputs":[],"source":["hist = transformer.history.history\n","\n","time_and_date = running_time\n","number_of_text_pairs =  len(text_pairs)\n","training_loss = hist['loss']\n","training_accuracy = hist['accuracy']\n","val_loss = hist['val_loss']\n","val_accuracy = hist['val_accuracy']\n","\n","train_history = {\n","    'time_and_date': time_and_date,\n","    'number_of_text_pairs': number_of_text_pairs,\n","    'epochs': epochs,\n","    'training_loss': training_loss,\n","    'training_accuracy': training_accuracy,\n","    'val_loss': val_loss, \n","    'val_accuracy': val_accuracy\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zB89HIv3TWH_"},"outputs":[],"source":["import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMWdWRU-SSIu"},"outputs":[],"source":["project_folder = '/content/drive/MyDrive/Colab Notebooks/TensorFlow for Beginners/Natural Language Processing/ENG-KAL_translation/train_history'\n","history_file_name = title + '_' + f'train_history_{time_and_date}.json'\n","history_file = project_folder + '/' + history_file_name\n","\n","with open(history_file, 'w') as file_object:  #open the file in write mode\n"," json.dump(train_history, file_object)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59Wo5YMfOEsr"},"outputs":[],"source":["# # Uncoment this code to read the json file of interest. Rememeber to add the link\n","\n","# with open(history_file, 'r') as f:\n","#   data = json.load(f)\n","# data"]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/ENG-KAL_translation/models/eng_to_kal_transformer_202306081320.keras\"\n","model = load_model(model_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"id":"HBdpSkztI2CN","executionInfo":{"status":"error","timestamp":1686231319149,"user_tz":-180,"elapsed":592,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"}},"outputId":"9c004eed-0389-4219-9a0f-55528f522aaa"},"execution_count":31,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-9d74484aa859>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/ENG-KAL_translation/models/eng_to_kal_transformer_202306081320.keras\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/legacy/serialization.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;34mf\"Unknown {printable_module_name}: '{class_name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;34m\"Please ensure you are using a `keras.utils.custom_object_scope` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unknown layer: 'PositionalEmbedding'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
